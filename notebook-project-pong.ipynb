{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v0_08_Project_Pong.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:dqn-env] *",
      "language": "python",
      "name": "conda-env-dqn-env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ry25T3ZLhPPe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6raTgHndhPPh",
        "colab": {}
      },
      "source": [
        "#!pip install torch\n",
        "#!pip install gym\n",
        "# Necessary for LunarLander-v2\n",
        "#!apt install swig   # Only if installation of Box2d fails\n",
        "#!pip install 'gym[box-2d]'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ABInnLkmhPPm",
        "outputId": "4ca5088b-daa9-48ee-a8bc-5c58641a422c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from collections import deque\n",
        "from datetime import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "#import Box2D\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.grad as grad\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func\n",
        "\n",
        "use_gpu = True\n",
        "\n",
        "if torch.cuda.is_available() and use_gpu:\n",
        "    available_device = torch.device('cuda')\n",
        "    print(\"Using cuda\")\n",
        "else:\n",
        "    available_device = torch.device('cpu')\n",
        "    print(\"Using cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9A-v0BbLhPPq"
      },
      "source": [
        "## Class definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rQFtb8s_hPPr",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen = capacity)\n",
        "        \n",
        "    def sample(self, k):\n",
        "        return np.array(random.sample(self.buffer, k))\n",
        "    \n",
        "    def add(self, new_sample):\n",
        "        self.buffer.append(new_sample)\n",
        "        \n",
        "    def count(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden_1, n_hidden_2, n_hidden_3, n_output, learning_rate):\n",
        "        super(DQN, self).__init__()\n",
        "        if(n_hidden_3 > 0):        \n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(n_input, n_hidden_1).to(available_device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(n_hidden_1, n_hidden_2).to(available_device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(n_hidden_2, n_hidden_3).to(available_device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(n_hidden_3, n_output).to(available_device),\n",
        "            )\n",
        "        elif(n_hidden_2 > 0):        \n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(n_input, n_hidden_1).to(available_device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(n_hidden_1, n_hidden_2).to(available_device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(n_hidden_2, n_output).to(available_device),\n",
        "            )\n",
        "        else:\n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(n_input, n_hidden_1).to(available_device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(n_hidden_1, n_output).to(available_device),\n",
        "            )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.loss_fct = nn.SmoothL1Loss()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "    \n",
        "    def loss(self, q_outputs, q_targets):\n",
        "        #return 0.5 * torch.sum(torch.pow(q_outputs - q_targets, 2))\n",
        "        return self.loss_fct(q_outputs.float(), q_targets.float())\n",
        "        \n",
        "    def update_params(self, new_params, tau):\n",
        "        params = self.state_dict()\n",
        "        for k in params.keys():\n",
        "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
        "        self.load_state_dict(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItB-Mu12hPPz"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cvMJGVafhPP4",
        "outputId": "674bc87d-985f-459f-d58d-878f41cd3f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "run_in_colab = True\n",
        "load_networks = False\n",
        "\n",
        "if(run_in_colab):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    policy_net_path = \"/content/gdrive/My Drive/Colab Notebooks/pong-ram-policy.pt\"\n",
        "    target_net_path = \"/content/gdrive/My Drive/Colab Notebooks/pong-ram-target.pt\"\n",
        "else:\n",
        "    policy_net_path = \"/home/philipp/Dokumente/AAAUniversitaet/Deep-Learning/Reinforcement-learning/lunar-policy.pt\"\n",
        "    target_net_path = \"/home/philipp/Dokumente/AAAUniversitaet/Deep-Learning/Reinforcement-learning/lunar-target.pt\"\n",
        "\n",
        "# Setup environment\n",
        "env = gym.make(\"Pong-ramDeterministic-v4\")\n",
        "\n",
        "# Set seeds\n",
        "seed = 42\n",
        "np.random.seed(seed)  # Numpy module.\n",
        "random.seed(seed)  # Python random module.\n",
        "env.seed(seed)\n",
        "env.action_space.np_random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# Set hyperparameters\n",
        "num_epochs = 5000000\n",
        "batch_size = 64 #32\n",
        "learning_rate = 0.0001\n",
        "gamma = 0.99 #0.95\n",
        "replay_buffer_capacity = 1000000 #100000\n",
        "replay_init_size = 10000\n",
        "epsilon = 1.0\n",
        "epsilon_final = 0.05\n",
        "epsilon_final_reached = 100000 # ebert 50000 #100000\n",
        "epsilon_decay = (epsilon - epsilon_final)/epsilon_final_reached\n",
        "target_update_frequency = 5000 #1000\n",
        "# tau = 0.01\n",
        "validation_frequency = 10000\n",
        "save_frequency = 100000\n",
        "do_validation = False\n",
        "\n",
        "\n",
        "\n",
        "# Define policy and target networks\n",
        "n_input = env.observation_space.shape[0]\n",
        "n_hidden_1 = 512\n",
        "n_hidden_2 = 256\n",
        "n_hidden_3 = 64\n",
        "n_output = env.action_space.n\n",
        "\n",
        "\n",
        "policy_net = DQN(n_input, n_hidden_1, n_hidden_2, n_hidden_3, n_output, learning_rate)\n",
        "target_net = DQN(n_input, n_hidden_1, n_hidden_2, n_hidden_3, n_output, learning_rate)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "\n",
        "if(load_networks):\n",
        "    print(\"Loading saved networks from file\")\n",
        "    policy_net.load_state_dict(torch.load(policy_net_path))\n",
        "    target_net.load_state_dict(torch.load(target_net_path))\n",
        "\n",
        "    # We have a (somewhat) working net already -> Use network to prefill buffer\n",
        "    print(\"Prefilling replay buffer\")\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
        "    s = env.reset()\n",
        "    for i in range(replay_init_size):\n",
        "        with torch.no_grad():\n",
        "            s_tensor = torch.as_tensor(s, device = available_device).float()\n",
        "            a = policy_net.forward(s_tensor).argmax().item()\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        replay_buffer.add([s,a,s1,r,done])\n",
        "        s = s1\n",
        "        if(done):\n",
        "            s = env.reset()\n",
        "            done = False\n",
        "else:\n",
        "    # Prefill the replay buffer randomly\n",
        "    print(\"Prefilling replay buffer\")\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
        "    s = env.reset()\n",
        "    for i in range(replay_init_size):\n",
        "        a = env.action_space.sample()\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        replay_buffer.add([s,a,s1,r,done])\n",
        "        s = s1\n",
        "        if(done):\n",
        "            s = env.reset()\n",
        "            done = False\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training\")\n",
        "losses, rewards, episode_duration = [], [], []\n",
        "episode_loss, episode_reward, episode_it = 0, 0, 0\n",
        "completed_at_last_validation = 0\n",
        "s = env.reset()\n",
        "starttime = datetime.now()\n",
        "try:\n",
        "    for i in range(num_epochs):    \n",
        "        # Do one gradient step\n",
        "        batch = replay_buffer.sample(batch_size)\n",
        "        ss = torch.as_tensor(np.stack(batch[:,0]), device = available_device).float()\n",
        "        aa = torch.as_tensor(np.stack(batch[:,1]), device = available_device)\n",
        "        ss1 = torch.as_tensor(np.stack(batch[:,2]), device = available_device).float()\n",
        "        rr = torch.as_tensor(np.stack(batch[:,3]), device = available_device)\n",
        "        ddone = torch.as_tensor(np.stack(batch[:,4]), device = available_device)\n",
        "        \n",
        "        policy_net.optimizer.zero_grad()\n",
        "        Q = policy_net.forward(ss)\n",
        "        q_policy = Q[range(len(aa)), aa]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            q_target = rr + gamma * target_net.forward(ss1).max(dim=1)[0] * (~ ddone)\n",
        "            #aa1 = target_net.forward(ss1).argmax(dim=1)\n",
        "            #q_target = rr + gamma * target_net.forward(ss1)[range(len(aa1)), aa1] * (~ ddone)\n",
        "            \n",
        "        loss = policy_net.loss(q_policy, q_target)\n",
        "        loss.backward()\n",
        "        policy_net.optimizer.step()\n",
        "        \n",
        "        # Update target network parameters from policy network parameters\n",
        "        if((i+1)%target_update_frequency == 0):\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "        #target_net.update_params(policy_net.state_dict(), tau)\n",
        "\n",
        "        # Decrease epsilon\n",
        "        if(epsilon > epsilon_final):\n",
        "            epsilon -= epsilon_decay\n",
        "        \n",
        "        # Add new sample to buffer\n",
        "        if(np.random.uniform() < epsilon):\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                s_tensor = torch.as_tensor(s, device = available_device).float()\n",
        "                a = policy_net.forward(s_tensor).argmax().item()\n",
        "\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        replay_buffer.add([s, a, s1, r, done])\n",
        "        s = s1\n",
        "            \n",
        "        episode_it += 1\n",
        "        episode_loss += loss.item()\n",
        "        episode_reward += r\n",
        "        \n",
        "        if(done):\n",
        "            episode_duration.append(episode_it)\n",
        "            losses.append(episode_loss/episode_it)\n",
        "            rewards.append(episode_reward)\n",
        "            episode_loss, episode_reward, episode_it = 0, 0, 0\n",
        "            done_any = False\n",
        "            s = env.reset()\n",
        "\n",
        "        if ((i+1)%validation_frequency == 0):\n",
        "            if do_validation:\n",
        "                validation_rewards, validation_duration = [], []\n",
        "                episode_reward, episode_it = 0, 0\n",
        "                s = env.reset()\n",
        "                k = 0\n",
        "                while k < 10:\n",
        "                    with torch.no_grad():\n",
        "                        s_tensor = torch.as_tensor(s, device = available_device).float()\n",
        "                        a = policy_net.forward(s_tensor).argmax().item()\n",
        "                    s1, r, done, _ = env.step(a)\n",
        "                    episode_reward += r\n",
        "                    episode_it += 1\n",
        "                    s = s1\n",
        "                    if(done):\n",
        "                        validation_duration.append(episode_it)\n",
        "                        validation_rewards.append(episode_reward)\n",
        "                        episode_reward, episode_it = 0, 0\n",
        "                        done = False\n",
        "                        k += 1\n",
        "                        s = env.reset()\n",
        "                \n",
        "                print(\"%i: Episodes completed: %d \\t Mean training reward: %5.2f \\t Mean validation reward: %5.2f \\t Mean normalized loss: %5.2f \\t Mean training duration: %5.2f \\t Mean validation duration: %5.2f\" % \n",
        "                    (i+1, len(rewards[completed_at_last_validation:]), np.mean(rewards[completed_at_last_validation:]), np.mean(validation_rewards), np.mean(losses[completed_at_last_validation:]), np.mean(episode_duration[completed_at_last_validation:]), np.mean(validation_duration)))\n",
        "            else:\n",
        "                print(\"%i: Episodes completed: %d \\t Mean training reward: %5.2f \\t Mean normalized loss: %5.2f \\t Mean training duration: %5.2f\" % \n",
        "                    (i+1, len(rewards[completed_at_last_validation:]), np.mean(rewards[completed_at_last_validation:]), np.mean(losses[completed_at_last_validation:]), np.mean(episode_duration[completed_at_last_validation:])))\n",
        "            \n",
        "            completed_at_last_validation = len(rewards)\n",
        "        \n",
        "        # Save the networks intermittently\n",
        "        if((i+1)%save_frequency == 0):\n",
        "            torch.save(policy_net.state_dict(), policy_net_path)\n",
        "            torch.save(target_net.state_dict(), target_net_path)            \n",
        "except KeyboardInterrupt:\n",
        "    print('Training interrupted early.')\n",
        "\n",
        "torch.save(policy_net.state_dict(), policy_net_path)\n",
        "torch.save(target_net.state_dict(), target_net_path)   \n",
        "\n",
        "endtime = datetime.now()\n",
        "print(\"Finished training. Completed %d episodes in %s.\" % (len(rewards), str(endtime - starttime)))\n",
        "\n",
        "\n",
        "# Run tests:\n",
        "try:\n",
        "    print(\"\\nRunning tests\")\n",
        "    test_rewards, test_duration = [], []\n",
        "    episode_reward, episode_it = 0, 0\n",
        "    s = env.reset()\n",
        "    k, nr_tests = 0, 10\n",
        "    while k < nr_tests:\n",
        "        with torch.no_grad():\n",
        "            s_tensor = torch.as_tensor(s, device = available_device).float()\n",
        "            a = policy_net.forward(s_tensor).argmax().item()\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        episode_reward += r\n",
        "        episode_it += 1\n",
        "        s = s1\n",
        "        if(done):\n",
        "            test_duration.append(episode_it)\n",
        "            test_rewards.append(episode_reward)\n",
        "            episode_reward, episode_it = 0, 0\n",
        "            done = False\n",
        "            k += 1\n",
        "            s = env.reset()\n",
        "except KeyboardInterrupt:\n",
        "    print('Testing interrupted early.')    \n",
        "                \n",
        "print(\"Mean test reward: %5.2f \\t Mean test duration: %5.2f\" % (np.mean(test_rewards), np.mean(test_duration)))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Prefilling replay buffer\n",
            "Starting training\n",
            "10000: Episodes completed: 11 \t Mean training reward: -20.55 \t Mean normalized loss:  0.05 \t Mean training duration: 908.91\n",
            "20000: Episodes completed: 11 \t Mean training reward: -20.36 \t Mean normalized loss:  0.01 \t Mean training duration: 900.91\n",
            "30000: Episodes completed: 11 \t Mean training reward: -20.73 \t Mean normalized loss:  0.01 \t Mean training duration: 846.45\n",
            "40000: Episodes completed: 12 \t Mean training reward: -20.50 \t Mean normalized loss:  0.01 \t Mean training duration: 889.08\n",
            "50000: Episodes completed: 11 \t Mean training reward: -20.45 \t Mean normalized loss:  0.01 \t Mean training duration: 887.64\n",
            "60000: Episodes completed: 10 \t Mean training reward: -19.70 \t Mean normalized loss:  0.01 \t Mean training duration: 1030.80\n",
            "70000: Episodes completed: 9 \t Mean training reward: -19.89 \t Mean normalized loss:  0.01 \t Mean training duration: 1047.56\n",
            "80000: Episodes completed: 9 \t Mean training reward: -19.33 \t Mean normalized loss:  0.01 \t Mean training duration: 1101.67\n",
            "90000: Episodes completed: 9 \t Mean training reward: -19.33 \t Mean normalized loss:  0.01 \t Mean training duration: 1107.56\n",
            "100000: Episodes completed: 7 \t Mean training reward: -18.57 \t Mean normalized loss:  0.01 \t Mean training duration: 1511.14\n",
            "110000: Episodes completed: 6 \t Mean training reward: -18.83 \t Mean normalized loss:  0.01 \t Mean training duration: 1534.67\n",
            "120000: Episodes completed: 6 \t Mean training reward: -19.00 \t Mean normalized loss:  0.01 \t Mean training duration: 1642.83\n",
            "130000: Episodes completed: 6 \t Mean training reward: -18.83 \t Mean normalized loss:  0.01 \t Mean training duration: 1665.83\n",
            "140000: Episodes completed: 5 \t Mean training reward: -16.60 \t Mean normalized loss:  0.01 \t Mean training duration: 2044.20\n",
            "150000: Episodes completed: 5 \t Mean training reward: -19.00 \t Mean normalized loss:  0.01 \t Mean training duration: 1787.60\n",
            "160000: Episodes completed: 5 \t Mean training reward: -17.00 \t Mean normalized loss:  0.01 \t Mean training duration: 2333.20\n",
            "170000: Episodes completed: 4 \t Mean training reward: -17.25 \t Mean normalized loss:  0.01 \t Mean training duration: 2244.50\n",
            "180000: Episodes completed: 4 \t Mean training reward: -17.50 \t Mean normalized loss:  0.01 \t Mean training duration: 2526.75\n",
            "190000: Episodes completed: 3 \t Mean training reward: -13.67 \t Mean normalized loss:  0.01 \t Mean training duration: 3119.33\n",
            "200000: Episodes completed: 4 \t Mean training reward: -15.25 \t Mean normalized loss:  0.01 \t Mean training duration: 2816.00\n",
            "210000: Episodes completed: 3 \t Mean training reward: -14.33 \t Mean normalized loss:  0.01 \t Mean training duration: 3338.67\n",
            "220000: Episodes completed: 3 \t Mean training reward: -17.33 \t Mean normalized loss:  0.01 \t Mean training duration: 2774.67\n",
            "230000: Episodes completed: 4 \t Mean training reward: -15.75 \t Mean normalized loss:  0.01 \t Mean training duration: 2975.75\n",
            "240000: Episodes completed: 3 \t Mean training reward: -15.67 \t Mean normalized loss:  0.01 \t Mean training duration: 3109.67\n",
            "250000: Episodes completed: 3 \t Mean training reward: -13.00 \t Mean normalized loss:  0.01 \t Mean training duration: 3279.67\n",
            "260000: Episodes completed: 3 \t Mean training reward: -15.00 \t Mean normalized loss:  0.01 \t Mean training duration: 3262.33\n",
            "270000: Episodes completed: 3 \t Mean training reward: -14.00 \t Mean normalized loss:  0.01 \t Mean training duration: 2993.67\n",
            "280000: Episodes completed: 4 \t Mean training reward: -16.25 \t Mean normalized loss:  0.01 \t Mean training duration: 3066.50\n",
            "290000: Episodes completed: 3 \t Mean training reward: -13.33 \t Mean normalized loss:  0.01 \t Mean training duration: 2494.67\n",
            "300000: Episodes completed: 4 \t Mean training reward: -15.50 \t Mean normalized loss:  0.01 \t Mean training duration: 2497.50\n",
            "310000: Episodes completed: 3 \t Mean training reward: -9.00 \t Mean normalized loss:  0.01 \t Mean training duration: 3575.00\n",
            "320000: Episodes completed: 4 \t Mean training reward: -14.00 \t Mean normalized loss:  0.01 \t Mean training duration: 2611.25\n",
            "330000: Episodes completed: 3 \t Mean training reward: -12.33 \t Mean normalized loss:  0.01 \t Mean training duration: 3173.67\n",
            "340000: Episodes completed: 3 \t Mean training reward: -12.00 \t Mean normalized loss:  0.01 \t Mean training duration: 3019.67\n",
            "350000: Episodes completed: 4 \t Mean training reward: -11.75 \t Mean normalized loss:  0.00 \t Mean training duration: 2716.00\n",
            "360000: Episodes completed: 4 \t Mean training reward: -14.50 \t Mean normalized loss:  0.00 \t Mean training duration: 2942.00\n",
            "370000: Episodes completed: 3 \t Mean training reward: -14.33 \t Mean normalized loss:  0.00 \t Mean training duration: 3100.00\n",
            "380000: Episodes completed: 3 \t Mean training reward: -13.33 \t Mean normalized loss:  0.00 \t Mean training duration: 3396.67\n",
            "390000: Episodes completed: 3 \t Mean training reward: -14.00 \t Mean normalized loss:  0.00 \t Mean training duration: 3189.33\n",
            "400000: Episodes completed: 3 \t Mean training reward: -14.00 \t Mean normalized loss:  0.00 \t Mean training duration: 3481.33\n",
            "410000: Episodes completed: 3 \t Mean training reward: -15.33 \t Mean normalized loss:  0.00 \t Mean training duration: 2798.67\n",
            "420000: Episodes completed: 4 \t Mean training reward: -15.00 \t Mean normalized loss:  0.00 \t Mean training duration: 2835.75\n",
            "430000: Episodes completed: 3 \t Mean training reward: -14.00 \t Mean normalized loss:  0.01 \t Mean training duration: 2969.33\n",
            "440000: Episodes completed: 4 \t Mean training reward: -15.00 \t Mean normalized loss:  0.00 \t Mean training duration: 2630.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTzfofXXS0_K"
      },
      "source": [
        "## Some old timing test\n",
        "\n",
        "from datetime import datetime\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
        "starttime = datetime.now()\n",
        "while True:\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    for j in range(episode_length):\n",
        "        a = env.action_space.sample()\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        replay_buffer.add([s,a,s1,r,done])\n",
        "        s = s1\n",
        "        i = i+1\n",
        "        if(done):\n",
        "            break\n",
        "        if(i >= replay_buffer_capacity):\n",
        "            break;\n",
        "    if(i >= replay_buffer_capacity):\n",
        "            break;\n",
        "\n",
        "print(\"Creation time on CPU: \", datetime.now() - starttime)\n",
        "starttime = datetime.now()\n",
        "\n",
        "for i in range(10000):\n",
        "    batch = replay_buffer.sample(batch_size)\n",
        "    ss = torch.as_tensor(np.stack(batch[:,0]), device = available_device)\n",
        "    aa = torch.as_tensor(np.stack(batch[:,1]), device = available_device)\n",
        "    ss1 = torch.as_tensor(np.stack(batch[:,2]), device = available_device)\n",
        "    rr = torch.as_tensor(np.stack(batch[:,3]), device = available_device)\n",
        "    ddone = torch.as_tensor(np.stack(batch[:,4]), device = available_device)\n",
        "\n",
        "print(\"Sampling time with copy to GPU: \", datetime.now() - starttime)\n",
        "starttime = datetime.now()\n",
        "\n",
        "replay_buffer = ReplayBuffer2(replay_buffer_capacity)\n",
        "starttime = datetime.now()\n",
        "while True:\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    for j in range(episode_length):\n",
        "        a = env.action_space.sample()\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        s = torch.as_tensor(s, device = available_device)\n",
        "        a = torch.as_tensor(a, device = available_device)\n",
        "        s1 = torch.as_tensor(s1, device = available_device)\n",
        "        r = torch.as_tensor(r, device = available_device)\n",
        "        done = torch.as_tensor(done, device = available_device)\n",
        "        replay_buffer.add([s,a,s1,r,done])\n",
        "        s = s1\n",
        "        i = i+1\n",
        "        if(done.item()):\n",
        "            break\n",
        "        if(i >= replay_buffer_capacity):\n",
        "            break;\n",
        "    if(i >= replay_buffer_capacity):\n",
        "            break;\n",
        "\n",
        "print(\"Creation time on GPU: \", datetime.now() - starttime)\n",
        "starttime = datetime.now()\n",
        "\n",
        "for i in range(10000):\n",
        "    batch = replay_buffer.sample(batch_size)\n",
        "    batch = [list(x) for x in zip(*batch)]\n",
        "    ss = torch.stack(batch[0])\n",
        "    aa = torch.stack(batch[1])\n",
        "    ss1 = torch.stack(batch[2])\n",
        "    rr = torch.stack(batch[3])\n",
        "    ddone = torch.stack(batch[4])\n",
        "\n",
        "print(\"Sampling time directly on GPU: \", datetime.now() - starttime)"
      ]
    }
  ]
}